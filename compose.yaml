name: AI_chat

networks:
  ai_chat_network: # Definicja sieci
    driver: bridge

volumes:
  ollama:

services:
  ollama:
    container_name: ollama
    image: ollama/ollama
    entrypoint: ["/usr/bin/bash", "/entrypoint.sh"]
    networks:
      - ai_chat_network
    volumes:
      - ollama:/root/.ollama
      - ./ollama/entrypoint.sh:/entrypoint.sh
    ports:
      - 11434:11434
    env_file:
      - "models.env"
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities:
    #             - gpu

    pull_policy: always
    tty: true
    restart: always

  ai_api:
    container_name: ai_api
    depends_on:
      - ollama
    ports:
      - 8001:8001
    environment:
      - ollama_url=http://ollama:11434
    env_file:
      - "models.env"
    build:
      context: ./API
      dockerfile: Dockerfile
    networks:
      - ai_chat_network
    tty: true
    volumes:
      - ./API:/python-docker

  frontend:
    depends_on:
      - ai_api
    build:
      context: ./Front
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    container_name: chatatins-front
